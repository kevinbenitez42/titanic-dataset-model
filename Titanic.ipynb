{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_NumericColumn(key='Age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _NumericColumn(key='SibSp', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _NumericColumn(key='Parch', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _NumericColumn(key='Fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='Pclass', vocabulary_list=(1, 2, 3), dtype=tf.int64, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='Sex', vocabulary_list=('female', 'male'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='Embarked', vocabulary_list=('S', 'Q', 'C'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmphdr2d0w2\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmphdr2d0w2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6f7ac62278>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmphdr2d0w2/model.ckpt.\n",
      "INFO:tensorflow:loss = 153.746, step = 1\n",
      "INFO:tensorflow:global_step/sec: 758.451\n",
      "INFO:tensorflow:loss = 33.6359, step = 101 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.95\n",
      "INFO:tensorflow:loss = 22.5603, step = 201 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1053.67\n",
      "INFO:tensorflow:loss = 23.3858, step = 301 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 971.176\n",
      "INFO:tensorflow:loss = 26.5379, step = 401 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1088.93\n",
      "INFO:tensorflow:loss = 16.0031, step = 501 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1144.41\n",
      "INFO:tensorflow:loss = 18.5899, step = 601 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 980.046\n",
      "INFO:tensorflow:loss = 20.9292, step = 701 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.158\n",
      "INFO:tensorflow:loss = 19.6268, step = 801 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 862.697\n",
      "INFO:tensorflow:loss = 22.6632, step = 901 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 737.769\n",
      "INFO:tensorflow:loss = 23.098, step = 1001 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 751.833\n",
      "INFO:tensorflow:loss = 22.4814, step = 1101 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 743.5\n",
      "INFO:tensorflow:loss = 20.5226, step = 1201 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 896.002\n",
      "INFO:tensorflow:loss = 21.243, step = 1301 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 897.419\n",
      "INFO:tensorflow:loss = 14.7891, step = 1401 (0.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 984.193\n",
      "INFO:tensorflow:loss = 17.2509, step = 1501 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 775.257\n",
      "INFO:tensorflow:loss = 9.75416, step = 1601 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 738.762\n",
      "INFO:tensorflow:loss = 11.9322, step = 1701 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 747.415\n",
      "INFO:tensorflow:loss = 21.0672, step = 1801 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 874.695\n",
      "INFO:tensorflow:loss = 18.2551, step = 1901 (0.114 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmphdr2d0w2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.61997.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-31-02:19:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmphdr2d0w2/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-31-02:19:59\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.754717, accuracy_baseline = 0.591195, auc = 0.777823, auc_precision_recall = 0.790671, average_loss = 0.588119, global_step = 2000, label/mean = 0.408805, loss = 46.7555, prediction/mean = 0.388328\n",
      "\n",
      "Test set accuracy: 0.755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from   sklearn.preprocessing import LabelBinarizer\n",
    "from   sklearn.preprocessing import LabelEncoder\n",
    "from   sklearn.preprocessing import Imputer\n",
    "import numpy \n",
    "from   collections import Counter\n",
    "\n",
    "\n",
    "from IPython.core.display import display\n",
    "\n",
    "\n",
    "columns = [\"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \n",
    "           \"Sex\", \"Age\", \"SibSp\", \"Parch\", \n",
    "           \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\n",
    "\n",
    "#Same as columns but with \"Survived\" removed\n",
    "features =  [\"PassengerId\", \"Pclass\", \"Name\", \n",
    "           \"Sex\", \"Age\", \"SibSp\", \"Parch\", \n",
    "           \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\n",
    "\n",
    "labels   = 'Survived'\n",
    "\n",
    "cat_features = [\"Pclass\",\"Sex\",\"Embarked\"]\n",
    "\n",
    "num_features   = [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\n",
    "\n",
    "useless_features = [\"PassengerId\", \"Name\",\"Ticket\",\"Cabin\"]\n",
    "\n",
    "labels   = \"Survived\"\n",
    "\n",
    "def main():\n",
    "    data_set       = pd.read_csv(\"train.csv\")                #download training/training set\n",
    "    validation_set = pd.read_csv(\"test.csv\")\n",
    "    data_set       = data_set.drop(useless_features, axis=1) #drop useless features\n",
    "    data_set       = data_set.dropna()                       #drop rows with missing data         \n",
    "    training_set, test_set = split_frame(data_set)\n",
    "    \n",
    "    training_features, training_labels = training_set, training_set.pop(labels)\n",
    "    test_features, test_labels = test_set, test_set.pop(labels)\n",
    "    \n",
    "    classifier        = model(feature_columns(data_set))\n",
    "    \n",
    "    training_features = df_to_dict_list(training_features)\n",
    "    test_features = df_to_dict_list(test_features)\n",
    "    classifier.train(\n",
    "        input_fn=lambda:training_input_fn(training_features, training_labels,50),\n",
    "        steps=2000)\n",
    "    eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(test_features, test_labels, 100))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "def unique_val(data_set,feature_name):\n",
    "    my_list=[]\n",
    "    for i in set(data_set[feature_name].tolist()):\n",
    "        my_list.append(i)\n",
    "    return my_list\n",
    "\n",
    "def training_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(buffer_size=100).repeat(count=None).batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "def df_to_dict_list(data_set):\n",
    "    my_list={}\n",
    "    for column in data_set.columns.values:\n",
    "        my_list[column] = data_set[column].tolist()\n",
    "    return my_list\n",
    "\n",
    "def feature_columns(data_set, cat_features=cat_features, \\\n",
    "                    num_features=num_features):\n",
    "    \n",
    "    numeric_feature_column = []\n",
    "    for feature in num_features:\n",
    "        numeric_feature_column.append(tf.feature_column.numeric_column(\n",
    "            key=feature,\n",
    "            dtype=tf.float32))\n",
    "    \n",
    "    categorical_feature_column = []\n",
    "    for feature in cat_features:\n",
    "        categorical_feature_column.append(tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key=feature,\n",
    "            vocabulary_list=unique_val(data_set,feature)\n",
    "        ))\n",
    "    \n",
    "    indicator_columns=[]\n",
    "    for column in categorical_feature_column:\n",
    "        indicator_columns.append(tf.feature_column.indicator_column(column))\n",
    "    feature_columns = numeric_feature_column  + indicator_columns\n",
    "    print(feature_columns)\n",
    "  \n",
    "    return feature_columns\n",
    "\n",
    "\n",
    "def encode_columns(data_set, cat_features=cat_features):\n",
    "    for feature in cat_features:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_binarizer = LabelBinarizer()\n",
    "        \n",
    "        label_encoder.fit(data_set[feature].tolist())\n",
    "        encoded_train = label_encoder.transform(data_set[feature].tolist())\n",
    "        n_train = data_set.columns[data_set.columns.get_loc(feature)]\n",
    "\n",
    "        data_set.drop(n_train, axis=1, inplace=True)\n",
    "        label_binarizer.fit(encoded_train.tolist())\n",
    "        data_set[n_train] = label_binarizer.transform(encoded_train.tolist()).tolist()\n",
    "\n",
    "    \n",
    "\n",
    "def model(feature_columns):\n",
    "    return tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[10,10],\n",
    "    optimizer=tf.train.FtrlOptimizer(\n",
    "        learning_rate=0.1,\n",
    "        l1_regularization_strength=1.0,\n",
    "        l2_regularization_strength=1.0),\n",
    "        activation_fn=tf.nn.relu,\n",
    "    n_classes=2,\n",
    "    )\n",
    "\n",
    "def split_frame(data_set):\n",
    "    mask = np.random.rand(len(data_set)) < 0.8\n",
    "    training_set = data_set[mask]\n",
    "    test_set     = data_set[~mask]\n",
    "    return training_set, test_set\n",
    "    \n",
    "    \n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
